{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informal-damage",
   "metadata": {},
   "source": [
    "### Convolutional Block Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "upset-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from jax.numpy.fft import fft2, ifft2\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "from flax import serialization\n",
    "import sys\n",
    "from model import ResNet_CBAM\n",
    "sys.path.append('../..')\n",
    "from bvex_dl import *\n",
    "from namelist_dl import *\n",
    "from model import *\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "magnetic-extent",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def MSE(x, y):\n",
    "    return jnp.mean((x-y)**2)\n",
    "\n",
    "\n",
    "def get_initial_params(key, model):\n",
    "    init_val = jnp.ones((2,64,64,3), jnp.float32)\n",
    "    initial_params = model().init(key, init_val)['params']\n",
    "    return initial_params\n",
    "\n",
    "\n",
    "def create_optimizer(params, learning_rate):\n",
    "    optimizer_def = optim.Adam(learning_rate=learning_rate)\n",
    "    optimizer = optimizer_def.create(params)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_datasets(batch_size):\n",
    "    ds_builder = tfds.builder('highres_forcing')\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train[:95%]', batch_size=batch_size, shuffle_files=True))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='train[5%:]', batch_size=batch_size, shuffle_files=True))\n",
    "    return train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e13b222-f04b-4300-8c97-d0068dd90d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(optimizer, batch):\n",
    "\n",
    "    def accumulate_mse(params):\n",
    "        \n",
    "        batch['states'] = jnp.transpose(batch['states'], axes=[0,1,3,2,4])\n",
    "        \n",
    "        stateNow = batch['states'][:,0,:,:,:]\n",
    "        qNow = batch['states'][:,0,:,:,0]\n",
    "        tNow = batch['time'][:, 0]\n",
    "        \n",
    "        qNew, tNew= vetdrk4(qNow, tNow)\n",
    "        pNew, _, _ = laplacian(qNew)\n",
    "        fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "        stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "        \n",
    "        del fNew\n",
    "        del pNew\n",
    "        \n",
    "        correction = model().apply({'params': params}, stateNew)\n",
    "        \n",
    "        del stateNew\n",
    "        \n",
    "        qNewCorrected = qNew + correction.squeeze()\n",
    "        pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "                \n",
    "        del qNew\n",
    "        \n",
    "        statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "        \n",
    "        loss = MSE(statesCorrected, batch['states'][:, 1, :, :, :2])\n",
    "        \n",
    "        del statesCorrected\n",
    "        \n",
    "        for i in range(1, n_look_ahead - 1):\n",
    "            \n",
    "            qNew, tNew = vetdrk4(qNewCorrected, tNew)\n",
    "            pNew, _, _ = laplacian(qNew)\n",
    "            fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "            stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "            \n",
    "            del pNew\n",
    "            del fNew\n",
    "            \n",
    "            correction = model().apply({'params': params}, stateNew)\n",
    "            \n",
    "            del stateNew\n",
    "            \n",
    "            qNewCorrected = qNew + correction.squeeze()\n",
    "            pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "            \n",
    "            del qNew\n",
    "            \n",
    "            statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "\n",
    "            loss += weight ** i * MSE(statesCorrected, batch['states'][:, i+1, :, :, :2])\n",
    "            \n",
    "        loss = jnp.mean(loss)\n",
    "        \n",
    "        # regularization\n",
    "        return loss, statesCorrected\n",
    "\n",
    "\n",
    "    grad_fn = jax.value_and_grad(accumulate_mse, has_aux=True)\n",
    "    (total_mse, final_state), grad = grad_fn(optimizer.target)\n",
    "    optimizer = optimizer.apply_gradient(grad)\n",
    "    final_mse = MSE(final_state, batch['states'][:, -1, :, :, :2])\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_mse\": total_mse,\n",
    "        \"final_mse\": final_mse\n",
    "    }\n",
    "    \n",
    "    del final_state\n",
    "    del total_mse\n",
    "    del final_mse\n",
    "    \n",
    "    return optimizer, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94e780b-f150-4ccd-b59f-0b769059bf87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    def accumulate_mse(params):\n",
    "        \n",
    "        batch['states'] = jnp.transpose(batch['states'], axes=[0,1,3,2,4])\n",
    "        \n",
    "        stateNow = batch['states'][:,0,:,:,:]\n",
    "        qNow = batch['states'][:,0,:,:,0]\n",
    "        tNow = batch['time'][:, 0]\n",
    "        \n",
    "        qNew, tNew= vetdrk4(qNow, tNow)\n",
    "        pNew, _, _ = laplacian(qNew)\n",
    "        fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "        stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "        \n",
    "        del fNew\n",
    "        del pNew\n",
    "        \n",
    "        correction = model().apply({'params': params}, stateNew)\n",
    "        \n",
    "        del stateNew\n",
    "        \n",
    "        qNewCorrected = qNew + correction.squeeze()\n",
    "        pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "                \n",
    "        del qNew\n",
    "        \n",
    "        statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "        \n",
    "        loss = MSE(statesCorrected, batch['states'][:, 1, :, :, :2])\n",
    "        \n",
    "        del statesCorrected\n",
    "        \n",
    "        for i in range(1, n_look_ahead - 1):\n",
    "            \n",
    "            qNew, tNew = vetdrk4(qNewCorrected, tNew)\n",
    "            pNew, _, _ = laplacian(qNew)\n",
    "            fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "            stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "            \n",
    "            del pNew\n",
    "            del fNew\n",
    "            \n",
    "            correction = model().apply({'params': params}, stateNew)\n",
    "            \n",
    "            del stateNew\n",
    "            \n",
    "            qNewCorrected = qNew + correction.squeeze()\n",
    "            pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "            \n",
    "            del qNew\n",
    "            \n",
    "            statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "\n",
    "            loss += weight ** i * MSE(statesCorrected, batch['states'][:, i+1, :, :, :2])\n",
    "            \n",
    "        loss = jnp.mean(loss)\n",
    "        \n",
    "        return loss, statesCorrected\n",
    "\n",
    "\n",
    "    total_mse, final_states = accumulate_mse(params)\n",
    "    final_mse = MSE(final_states, batch['states'][:, -1, :, :, :2])\n",
    "    \n",
    "    del final_states\n",
    "    \n",
    "    \n",
    "    metrics = {\n",
    "        \"total_mse\": total_mse,\n",
    "        \"final_mse\": final_mse\n",
    "\n",
    "    }\n",
    "    \n",
    "    del total_mse\n",
    "    del final_mse\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "following-finish",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(optimizer, train_ds, batch_size, epoch, rng):\n",
    "    \n",
    "    batch_metrics = []\n",
    "    # 4 cycles per epoch\n",
    "  \n",
    "    \n",
    "    for batch in train_ds:\n",
    "        optimizer, metrics = train_step(optimizer, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "        \n",
    "    training_batch_metrics = jax.device_get(batch_metrics)\n",
    "    \n",
    "    training_batch_metrics = training_batch_metrics[:-1]\n",
    "    \n",
    "    training_epoch_metrics = {\n",
    "        k: np.mean([metrics[k] for metrics in training_batch_metrics])\n",
    "        for k in training_batch_metrics[0]\n",
    "    }\n",
    "    \n",
    "    print('Training - epoch: %d, final step mse: %.4f, total mse: %.4f' \n",
    "          % (epoch,\n",
    "             training_epoch_metrics['final_mse'],\n",
    "             training_epoch_metrics['total_mse']))\n",
    "    \n",
    "    return optimizer, training_epoch_metrics\n",
    "\n",
    "\n",
    "def eval_model(model, test_ds):\n",
    "    \n",
    "    batch_metrics = []\n",
    "    for batch in test_ds:\n",
    "        metrics = eval_step(model, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "        \n",
    "    testing_batch_metrics = jax.device_get(batch_metrics)\n",
    "    \n",
    "    testing_batch_metrics = testing_batch_metrics[:-1]\n",
    "    \n",
    "    testing_metrics = {\n",
    "        k: np.mean([metrics[k] for metrics in testing_batch_metrics])\n",
    "        for k in testing_batch_metrics[0]\n",
    "    }\n",
    "\n",
    "    \n",
    "    print('Testing - epoch: %d,  final step mse: %.4f, total mse: %.4f'  \n",
    "          % (epoch,\n",
    "             testing_metrics['final_mse'],\n",
    "             testing_metrics['total_mse']))\n",
    "    \n",
    "    return testing_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d659a74d-276a-412c-bdfe-0d53f79d3d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - epoch: 1, final step mse: 0.0182, total mse: 0.2298\n",
      "Testing - epoch: 1,  final step mse: 0.0142, total mse: 0.1839\n",
      "Training - epoch: 2, final step mse: 0.0133, total mse: 0.1723\n",
      "Testing - epoch: 2,  final step mse: 0.0123, total mse: 0.1611\n",
      "Training - epoch: 3, final step mse: 0.0117, total mse: 0.1538\n",
      "Testing - epoch: 3,  final step mse: 0.0111, total mse: 0.1466\n",
      "Training - epoch: 4, final step mse: 0.0107, total mse: 0.1411\n",
      "Testing - epoch: 4,  final step mse: 0.0101, total mse: 0.1349\n",
      "Training - epoch: 5, final step mse: 0.0099, total mse: 0.1313\n",
      "Testing - epoch: 5,  final step mse: 0.0092, total mse: 0.1245\n",
      "Training - epoch: 6, final step mse: 0.0093, total mse: 0.1238\n",
      "Testing - epoch: 6,  final step mse: 0.0088, total mse: 0.1192\n",
      "Training - epoch: 7, final step mse: 0.0088, total mse: 0.1178\n",
      "Testing - epoch: 7,  final step mse: 0.0085, total mse: 0.1144\n",
      "Training - epoch: 8, final step mse: 0.0084, total mse: 0.1129\n",
      "Testing - epoch: 8,  final step mse: 0.0081, total mse: 0.1093\n",
      "Training - epoch: 9, final step mse: 0.0081, total mse: 0.1088\n",
      "Testing - epoch: 9,  final step mse: 0.0079, total mse: 0.1063\n",
      "Training - epoch: 10, final step mse: 0.0078, total mse: 0.1053\n",
      "Testing - epoch: 10,  final step mse: 0.0077, total mse: 0.1037\n",
      "Training - epoch: 11, final step mse: 0.0076, total mse: 0.1023\n",
      "Testing - epoch: 11,  final step mse: 0.0075, total mse: 0.1006\n",
      "Training - epoch: 12, final step mse: 0.0074, total mse: 0.0997\n",
      "Testing - epoch: 12,  final step mse: 0.0071, total mse: 0.0968\n",
      "Training - epoch: 13, final step mse: 0.0072, total mse: 0.0974\n",
      "Testing - epoch: 13,  final step mse: 0.0071, total mse: 0.0962\n",
      "Training - epoch: 14, final step mse: 0.0071, total mse: 0.0954\n",
      "Testing - epoch: 14,  final step mse: 0.0069, total mse: 0.0936\n",
      "Training - epoch: 15, final step mse: 0.0069, total mse: 0.0936\n",
      "Testing - epoch: 15,  final step mse: 0.0068, total mse: 0.0923\n",
      "Training - epoch: 16, final step mse: 0.0068, total mse: 0.0920\n",
      "Testing - epoch: 16,  final step mse: 0.0069, total mse: 0.0925\n",
      "Training - epoch: 17, final step mse: 0.0067, total mse: 0.0905\n",
      "Testing - epoch: 17,  final step mse: 0.0067, total mse: 0.0900\n",
      "Training - epoch: 18, final step mse: 0.0066, total mse: 0.0893\n",
      "Testing - epoch: 18,  final step mse: 0.0064, total mse: 0.0870\n",
      "Training - epoch: 19, final step mse: 0.0065, total mse: 0.0880\n",
      "Testing - epoch: 19,  final step mse: 0.0064, total mse: 0.0869\n",
      "Training - epoch: 20, final step mse: 0.0064, total mse: 0.0870\n",
      "Testing - epoch: 20,  final step mse: 0.0063, total mse: 0.0858\n",
      "Training - epoch: 21, final step mse: 0.0064, total mse: 0.0860\n",
      "Testing - epoch: 21,  final step mse: 0.0062, total mse: 0.0845\n",
      "Training - epoch: 22, final step mse: 0.0063, total mse: 0.0850\n",
      "Testing - epoch: 22,  final step mse: 0.0061, total mse: 0.0835\n",
      "Training - epoch: 23, final step mse: 0.0062, total mse: 0.0842\n",
      "Testing - epoch: 23,  final step mse: 0.0062, total mse: 0.0837\n",
      "Training - epoch: 24, final step mse: 0.0062, total mse: 0.0834\n",
      "Testing - epoch: 24,  final step mse: 0.0061, total mse: 0.0824\n",
      "Training - epoch: 25, final step mse: 0.0061, total mse: 0.0827\n",
      "Testing - epoch: 25,  final step mse: 0.0061, total mse: 0.0820\n",
      "Training - epoch: 26, final step mse: 0.0061, total mse: 0.0820\n",
      "Testing - epoch: 26,  final step mse: 0.0059, total mse: 0.0804\n",
      "Training - epoch: 27, final step mse: 0.0060, total mse: 0.0813\n",
      "Testing - epoch: 27,  final step mse: 0.0059, total mse: 0.0800\n",
      "Training - epoch: 28, final step mse: 0.0060, total mse: 0.0807\n",
      "Testing - epoch: 28,  final step mse: 0.0059, total mse: 0.0798\n",
      "Training - epoch: 29, final step mse: 0.0059, total mse: 0.0802\n",
      "Testing - epoch: 29,  final step mse: 0.0059, total mse: 0.0796\n",
      "Training - epoch: 30, final step mse: 0.0059, total mse: 0.0796\n",
      "Testing - epoch: 30,  final step mse: 0.0059, total mse: 0.0794\n",
      "Training - epoch: 31, final step mse: 0.0059, total mse: 0.0791\n",
      "Testing - epoch: 31,  final step mse: 0.0058, total mse: 0.0785\n",
      "Training - epoch: 32, final step mse: 0.0058, total mse: 0.0786\n",
      "Testing - epoch: 32,  final step mse: 0.0058, total mse: 0.0780\n",
      "Training - epoch: 33, final step mse: 0.0058, total mse: 0.0782\n",
      "Testing - epoch: 33,  final step mse: 0.0058, total mse: 0.0782\n",
      "Training - epoch: 34, final step mse: 0.0058, total mse: 0.0777\n",
      "Testing - epoch: 34,  final step mse: 0.0057, total mse: 0.0766\n",
      "Training - epoch: 35, final step mse: 0.0057, total mse: 0.0773\n",
      "Testing - epoch: 35,  final step mse: 0.0056, total mse: 0.0761\n",
      "Training - epoch: 36, final step mse: 0.0057, total mse: 0.0769\n",
      "Testing - epoch: 36,  final step mse: 0.0056, total mse: 0.0762\n",
      "Training - epoch: 37, final step mse: 0.0057, total mse: 0.0766\n",
      "Testing - epoch: 37,  final step mse: 0.0055, total mse: 0.0755\n",
      "Training - epoch: 38, final step mse: 0.0056, total mse: 0.0762\n",
      "Testing - epoch: 38,  final step mse: 0.0057, total mse: 0.0762\n",
      "Training - epoch: 39, final step mse: 0.0056, total mse: 0.0759\n",
      "Testing - epoch: 39,  final step mse: 0.0056, total mse: 0.0753\n",
      "Training - epoch: 40, final step mse: 0.0056, total mse: 0.0755\n",
      "Testing - epoch: 40,  final step mse: 0.0056, total mse: 0.0751\n",
      "Training - epoch: 41, final step mse: 0.0056, total mse: 0.0752\n",
      "Testing - epoch: 41,  final step mse: 0.0055, total mse: 0.0744\n",
      "Training - epoch: 42, final step mse: 0.0055, total mse: 0.0748\n",
      "Testing - epoch: 42,  final step mse: 0.0055, total mse: 0.0737\n",
      "Training - epoch: 43, final step mse: 0.0055, total mse: 0.0746\n",
      "Testing - epoch: 43,  final step mse: 0.0056, total mse: 0.0750\n",
      "Training - epoch: 44, final step mse: 0.0055, total mse: 0.0743\n",
      "Testing - epoch: 44,  final step mse: 0.0054, total mse: 0.0734\n",
      "Training - epoch: 45, final step mse: 0.0055, total mse: 0.0741\n",
      "Testing - epoch: 45,  final step mse: 0.0054, total mse: 0.0731\n",
      "Training - epoch: 46, final step mse: 0.0055, total mse: 0.0738\n",
      "Testing - epoch: 46,  final step mse: 0.0054, total mse: 0.0729\n",
      "Training - epoch: 47, final step mse: 0.0054, total mse: 0.0736\n",
      "Testing - epoch: 47,  final step mse: 0.0054, total mse: 0.0727\n",
      "Training - epoch: 48, final step mse: 0.0054, total mse: 0.0733\n",
      "Testing - epoch: 48,  final step mse: 0.0054, total mse: 0.0727\n",
      "Training - epoch: 49, final step mse: 0.0054, total mse: 0.0732\n",
      "Testing - epoch: 49,  final step mse: 0.0054, total mse: 0.0726\n",
      "Training - epoch: 50, final step mse: 0.0054, total mse: 0.0728\n",
      "Testing - epoch: 50,  final step mse: 0.0053, total mse: 0.0719\n",
      "Training - epoch: 51, final step mse: 0.0054, total mse: 0.0727\n",
      "Testing - epoch: 51,  final step mse: 0.0053, total mse: 0.0719\n",
      "Training - epoch: 52, final step mse: 0.0054, total mse: 0.0724\n",
      "Testing - epoch: 52,  final step mse: 0.0054, total mse: 0.0720\n",
      "Training - epoch: 53, final step mse: 0.0054, total mse: 0.0722\n",
      "Testing - epoch: 53,  final step mse: 0.0053, total mse: 0.0715\n",
      "Training - epoch: 54, final step mse: 0.0053, total mse: 0.0721\n",
      "Testing - epoch: 54,  final step mse: 0.0053, total mse: 0.0715\n",
      "Training - epoch: 55, final step mse: 0.0053, total mse: 0.0719\n",
      "Testing - epoch: 55,  final step mse: 0.0053, total mse: 0.0711\n",
      "Training - epoch: 56, final step mse: 0.0053, total mse: 0.0717\n",
      "Testing - epoch: 56,  final step mse: 0.0053, total mse: 0.0709\n",
      "Training - epoch: 57, final step mse: 0.0053, total mse: 0.0715\n",
      "Testing - epoch: 57,  final step mse: 0.0052, total mse: 0.0702\n",
      "Training - epoch: 58, final step mse: 0.0053, total mse: 0.0713\n",
      "Testing - epoch: 58,  final step mse: 0.0053, total mse: 0.0710\n",
      "Training - epoch: 59, final step mse: 0.0053, total mse: 0.0712\n",
      "Testing - epoch: 59,  final step mse: 0.0053, total mse: 0.0709\n",
      "Training - epoch: 60, final step mse: 0.0053, total mse: 0.0710\n",
      "Testing - epoch: 60,  final step mse: 0.0053, total mse: 0.0710\n",
      "Training - epoch: 61, final step mse: 0.0052, total mse: 0.0708\n",
      "Testing - epoch: 61,  final step mse: 0.0051, total mse: 0.0695\n",
      "Training - epoch: 62, final step mse: 0.0052, total mse: 0.0708\n",
      "Testing - epoch: 62,  final step mse: 0.0052, total mse: 0.0700\n",
      "Training - epoch: 63, final step mse: 0.0052, total mse: 0.0705\n",
      "Testing - epoch: 63,  final step mse: 0.0051, total mse: 0.0694\n",
      "Training - epoch: 64, final step mse: 0.0052, total mse: 0.0704\n",
      "Testing - epoch: 64,  final step mse: 0.0052, total mse: 0.0699\n",
      "Training - epoch: 65, final step mse: 0.0052, total mse: 0.0703\n",
      "Testing - epoch: 65,  final step mse: 0.0053, total mse: 0.0704\n",
      "Training - epoch: 66, final step mse: 0.0052, total mse: 0.0701\n",
      "Testing - epoch: 66,  final step mse: 0.0051, total mse: 0.0693\n",
      "Training - epoch: 67, final step mse: 0.0052, total mse: 0.0700\n",
      "Testing - epoch: 67,  final step mse: 0.0052, total mse: 0.0703\n",
      "Training - epoch: 68, final step mse: 0.0052, total mse: 0.0699\n",
      "Testing - epoch: 68,  final step mse: 0.0051, total mse: 0.0692\n",
      "Training - epoch: 69, final step mse: 0.0052, total mse: 0.0698\n",
      "Testing - epoch: 69,  final step mse: 0.0051, total mse: 0.0687\n",
      "Training - epoch: 70, final step mse: 0.0052, total mse: 0.0696\n",
      "Testing - epoch: 70,  final step mse: 0.0051, total mse: 0.0694\n",
      "Training - epoch: 71, final step mse: 0.0051, total mse: 0.0694\n",
      "Testing - epoch: 71,  final step mse: 0.0052, total mse: 0.0692\n",
      "Training - epoch: 72, final step mse: 0.0051, total mse: 0.0694\n",
      "Testing - epoch: 72,  final step mse: 0.0051, total mse: 0.0687\n",
      "Training - epoch: 73, final step mse: 0.0051, total mse: 0.0693\n",
      "Testing - epoch: 73,  final step mse: 0.0051, total mse: 0.0688\n",
      "Training - epoch: 74, final step mse: 0.0051, total mse: 0.0691\n",
      "Testing - epoch: 74,  final step mse: 0.0051, total mse: 0.0685\n",
      "Training - epoch: 75, final step mse: 0.0051, total mse: 0.0690\n",
      "Testing - epoch: 75,  final step mse: 0.0051, total mse: 0.0689\n",
      "Training - epoch: 76, final step mse: 0.0051, total mse: 0.0690\n",
      "Testing - epoch: 76,  final step mse: 0.0051, total mse: 0.0683\n",
      "Training - epoch: 77, final step mse: 0.0051, total mse: 0.0688\n",
      "Testing - epoch: 77,  final step mse: 0.0051, total mse: 0.0682\n",
      "Training - epoch: 78, final step mse: 0.0051, total mse: 0.0688\n",
      "Testing - epoch: 78,  final step mse: 0.0051, total mse: 0.0680\n",
      "Training - epoch: 79, final step mse: 0.0051, total mse: 0.0685\n",
      "Testing - epoch: 79,  final step mse: 0.0051, total mse: 0.0682\n",
      "Training - epoch: 80, final step mse: 0.0051, total mse: 0.0685\n",
      "Testing - epoch: 80,  final step mse: 0.0050, total mse: 0.0679\n",
      "Training - epoch: 81, final step mse: 0.0051, total mse: 0.0684\n",
      "Testing - epoch: 81,  final step mse: 0.0050, total mse: 0.0675\n",
      "Training - epoch: 82, final step mse: 0.0051, total mse: 0.0682\n",
      "Testing - epoch: 82,  final step mse: 0.0051, total mse: 0.0681\n",
      "Training - epoch: 83, final step mse: 0.0051, total mse: 0.0682\n",
      "Testing - epoch: 83,  final step mse: 0.0050, total mse: 0.0676\n",
      "Training - epoch: 84, final step mse: 0.0051, total mse: 0.0681\n",
      "Testing - epoch: 84,  final step mse: 0.0050, total mse: 0.0678\n",
      "Training - epoch: 85, final step mse: 0.0050, total mse: 0.0680\n",
      "Testing - epoch: 85,  final step mse: 0.0050, total mse: 0.0677\n",
      "Training - epoch: 86, final step mse: 0.0050, total mse: 0.0679\n",
      "Testing - epoch: 86,  final step mse: 0.0050, total mse: 0.0677\n",
      "Training - epoch: 87, final step mse: 0.0050, total mse: 0.0679\n",
      "Testing - epoch: 87,  final step mse: 0.0050, total mse: 0.0675\n",
      "Training - epoch: 88, final step mse: 0.0050, total mse: 0.0678\n",
      "Testing - epoch: 88,  final step mse: 0.0050, total mse: 0.0671\n",
      "Training - epoch: 89, final step mse: 0.0050, total mse: 0.0676\n",
      "Testing - epoch: 89,  final step mse: 0.0051, total mse: 0.0682\n",
      "Training - epoch: 90, final step mse: 0.0050, total mse: 0.0676\n",
      "Testing - epoch: 90,  final step mse: 0.0049, total mse: 0.0664\n",
      "Training - epoch: 91, final step mse: 0.0050, total mse: 0.0675\n",
      "Testing - epoch: 91,  final step mse: 0.0049, total mse: 0.0668\n",
      "Training - epoch: 92, final step mse: 0.0050, total mse: 0.0674\n",
      "Testing - epoch: 92,  final step mse: 0.0050, total mse: 0.0672\n",
      "Training - epoch: 93, final step mse: 0.0050, total mse: 0.0673\n",
      "Testing - epoch: 93,  final step mse: 0.0050, total mse: 0.0672\n",
      "Training - epoch: 94, final step mse: 0.0050, total mse: 0.0673\n",
      "Testing - epoch: 94,  final step mse: 0.0050, total mse: 0.0673\n",
      "Training - epoch: 95, final step mse: 0.0050, total mse: 0.0672\n",
      "Testing - epoch: 95,  final step mse: 0.0050, total mse: 0.0679\n",
      "Training - epoch: 96, final step mse: 0.0050, total mse: 0.0671\n",
      "Testing - epoch: 96,  final step mse: 0.0049, total mse: 0.0669\n",
      "Training - epoch: 97, final step mse: 0.0050, total mse: 0.0671\n",
      "Testing - epoch: 97,  final step mse: 0.0050, total mse: 0.0667\n",
      "Training - epoch: 98, final step mse: 0.0050, total mse: 0.0670\n",
      "Testing - epoch: 98,  final step mse: 0.0050, total mse: 0.0670\n",
      "Training - epoch: 99, final step mse: 0.0050, total mse: 0.0669\n",
      "Testing - epoch: 99,  final step mse: 0.0050, total mse: 0.0676\n",
      "Training - epoch: 100, final step mse: 0.0050, total mse: 0.0668\n",
      "Testing - epoch: 100,  final step mse: 0.0049, total mse: 0.0664\n"
     ]
    }
   ],
   "source": [
    "model = ResNet_CBAM\n",
    "rng = jax.random.PRNGKey(2021)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = get_initial_params(init_rng, model)\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 100\n",
    "batch_size = 4\n",
    "optimizer = create_optimizer(params, learning_rate=learning_rate)\n",
    "train_ds, test_ds = get_datasets(batch_size)\n",
    "weight = 1\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    optimizer, train_metrics = train_epoch(optimizer, train_ds, batch_size, epoch, input_rng)\n",
    "    eval_model(optimizer.target, test_ds)\n",
    "    dict_output = serialization.to_state_dict(optimizer.target)\n",
    "    np.save(f\"checkpoint/CBAM_state_dict_epoch_{epoch}.npy\", dict_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad674d69-40d0-4e58-9a8f-1602ed1101ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
