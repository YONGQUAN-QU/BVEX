{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statutory-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=.80\n"
     ]
    }
   ],
   "source": [
    "# GPU memory allocation \n",
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=.80\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "import flax\n",
    "from flax import serialization\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from bvex_dl import *\n",
    "from namelist_dl import *\n",
    "import time as Time\n",
    "from model import PeriodicCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "creative-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate):\n",
    "    \"\"\"Create initial `TrainState`.\"\"\"\n",
    "    cnn = PeriodicCNN()\n",
    "    params = cnn.init(rng, jnp.ones([1, 64, 64, 3]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n",
    "\n",
    "def get_datasets(batch_size):\n",
    "    ds_builder = tfds.builder('highres_forcing_long')\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train[:95%]', batch_size=batch_size, shuffle_files=True))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='train[95%:]', batch_size=batch_size, shuffle_files=True))\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "australian-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def accumulate_mse(params):\n",
    "        \n",
    "        batch['states'] = jnp.transpose(batch['states'], axes=[0,1,3,2,4])\n",
    "        \n",
    "        # obtain the initial state\n",
    "        stateNow = batch['states'][:,0,:,:,:]\n",
    "        qNow = batch['states'][:,0,:,:,0]\n",
    "        tNow = batch['time'][:, 0]\n",
    "        \n",
    "        qNew, tNew= vetdrk4(qNow, tNow)\n",
    "        pNew, _, _ = laplacian(qNew)\n",
    "        fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "        stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "        \n",
    "        correction = PeriodicCNN().apply({'params': params}, stateNew).squeeze()\n",
    "        \n",
    "        qNewCorrected = qNew + correction.squeeze()\n",
    "        pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "        \n",
    "        statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "        \n",
    "        loss = jnp.mean(optax.l2_loss(statesCorrected, batch['states'][:, 1, :, :, :2]))\n",
    "                       \n",
    "        for i in range(1, n_look_ahead - 1):\n",
    "            \n",
    "            qNew, tNew = vetdrk4(qNewCorrected, tNew)\n",
    "            pNew, _, _ = laplacian(qNew)\n",
    "            fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "            stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "            \n",
    "            correction = PeriodicCNN().apply({'params': params}, stateNew).squeeze()\n",
    "            \n",
    "            qNewCorrected = qNew + correction.squeeze()\n",
    "            pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "\n",
    "            statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "            \n",
    "            loss += weight ** i * jnp.mean(optax.l2_loss(statesCorrected, batch['states'][:, i+1, :, :, :2]))\n",
    "        \n",
    "        return loss, statesCorrected\n",
    "\n",
    "\n",
    "    grad_fn = jax.value_and_grad(accumulate_mse, has_aux=True)\n",
    "    (total_mse, final_state), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    final_mse = jnp.mean(optax.l2_loss(final_state, batch['states'][:, n_look_ahead, :, :, :2]))\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_mse\": total_mse,\n",
    "        \"final_mse\": final_mse\n",
    "    }\n",
    "    \n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "preliminary-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    def accumulate_mse(params):\n",
    "        \n",
    "        batch['states'] = jnp.transpose(batch['states'], axes=[0,1,3,2,4])\n",
    "        \n",
    "        stateNow = batch['states'][:,0,:,:,:]\n",
    "        qNow = batch['states'][:,0,:,:,0]\n",
    "        tNow = batch['time'][:, 0]\n",
    "        \n",
    "        qNew, tNew= vetdrk4(qNow, tNow)\n",
    "        pNew, _, _ = laplacian(qNew)\n",
    "        fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "        stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "        \n",
    "        correction = PeriodicCNN().apply({'params': params}, stateNew).squeeze()\n",
    "        \n",
    "        qNewCorrected = qNew + correction.squeeze()\n",
    "        pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "                \n",
    "        statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "        \n",
    "        loss = jnp.mean(optax.l2_loss(statesCorrected, batch['states'][:, 1, :, :, :2]))\n",
    "        \n",
    "        for i in range(1, n_look_ahead - 1):\n",
    "            \n",
    "            qNew, tNew = vetdrk4(qNewCorrected, tNew)\n",
    "            pNew, _, _ = laplacian(qNew)\n",
    "            fNew = jax.vmap(cal_forcing)(pNew, tNew)\n",
    "        \n",
    "            stateNew = jnp.stack((qNew, pNew, fNew), axis=-1)\n",
    "        \n",
    "            correction = PeriodicCNN().apply({'params': params}, stateNew).squeeze()\n",
    "            \n",
    "            qNewCorrected = qNew + correction.squeeze()\n",
    "            pNewCorrected, _, _ = laplacian(qNewCorrected)\n",
    "            \n",
    "            statesCorrected = jnp.stack((qNewCorrected, pNewCorrected), axis=-1)\n",
    "            \n",
    "            loss += weight ** i * jnp.mean(optax.l2_loss(statesCorrected, batch['states'][:, i+1, :, :, :2]))\n",
    "            \n",
    "        \n",
    "        return loss, statesCorrected\n",
    "\n",
    "\n",
    "    total_mse, final_states = accumulate_mse(params)\n",
    "    final_mse = jnp.mean(optax.l2_loss(final_states, batch['states'][:, 16, :, :, :2]))\n",
    "    \n",
    "    del final_states\n",
    "    \n",
    "    \n",
    "    metrics = {\n",
    "        \"total_mse\": total_mse,\n",
    "        \"final_mse\": final_mse\n",
    "\n",
    "    }\n",
    "    \n",
    "    del total_mse\n",
    "    del final_mse\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ambient-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \n",
    "    batch_metrics = []\n",
    "    # 4 cycles per epoch\n",
    "  \n",
    "    \n",
    "    for batch in train_ds:\n",
    "        state, metrics = train_step(state, batch)\n",
    "\n",
    "        batch_metrics.append(metrics)\n",
    "        \n",
    "    training_batch_metrics = jax.device_get(batch_metrics)\n",
    "    \n",
    "    training_batch_metrics = training_batch_metrics[:-1]\n",
    "    \n",
    "    training_epoch_metrics = {\n",
    "        k: np.mean([metrics[k] for metrics in training_batch_metrics])\n",
    "        for k in training_batch_metrics[0]\n",
    "    }\n",
    "    \n",
    "    print('Training - epoch: %d, final step mse: %.4f, total mse: %.4f' \n",
    "          % (epoch,\n",
    "             training_epoch_metrics['final_mse'],\n",
    "             training_epoch_metrics['total_mse']))\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def eval_model(params, test_ds):\n",
    "    \n",
    "    batch_metrics = []\n",
    "    for batch in test_ds:\n",
    "        metrics = eval_step(params, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "        \n",
    "    testing_batch_metrics = jax.device_get(batch_metrics)\n",
    "    \n",
    "    testing_batch_metrics = testing_batch_metrics[:-1]\n",
    "    \n",
    "    testing_metrics = {\n",
    "        k: np.mean([metrics[k] for metrics in testing_batch_metrics])\n",
    "        for k in testing_batch_metrics[0]\n",
    "    }\n",
    "\n",
    "    \n",
    "    print('Testing - epoch: %d,  final step mse: %.4f, total mse: %.4f'  \n",
    "          % (epoch,\n",
    "             testing_metrics['final_mse'],\n",
    "             testing_metrics['total_mse']))\n",
    "    \n",
    "    return testing_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increasing-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(2021)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "# learning rate scheduler \n",
    "schedule = optax.exponential_decay(init_value=0.001, \n",
    "                                   transition_steps=10,\n",
    "                                   decay_rate=0.8,\n",
    "                                   transition_begin=10,\n",
    "                                   staircase=True, \n",
    "                                    end_value=0.0002)\n",
    "\n",
    "#create train state\n",
    "learning_rate=schedule\n",
    "state = create_train_state(init_rng, learning_rate)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "#load data\n",
    "train_ds, test_ds = get_datasets(batch_size)\n",
    "weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rental-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - epoch: 1, final step mse: 0.0067, total mse: 0.0067\n",
      "Time - 25.244832515716553\n",
      "Testing - epoch: 1,  final step mse: 0.2234, total mse: 0.0047\n",
      "Training - epoch: 2, final step mse: 0.0044, total mse: 0.0044\n",
      "Time - 22.484177350997925\n",
      "Testing - epoch: 2,  final step mse: 0.2240, total mse: 0.0041\n",
      "Training - epoch: 3, final step mse: 0.0038, total mse: 0.0038\n",
      "Time - 22.57449245452881\n",
      "Testing - epoch: 3,  final step mse: 0.2241, total mse: 0.0037\n",
      "Training - epoch: 4, final step mse: 0.0034, total mse: 0.0034\n",
      "Time - 22.62535834312439\n",
      "Testing - epoch: 4,  final step mse: 0.2251, total mse: 0.0033\n",
      "Training - epoch: 5, final step mse: 0.0032, total mse: 0.0032\n",
      "Time - 22.702173233032227\n",
      "Testing - epoch: 5,  final step mse: 0.2260, total mse: 0.0031\n",
      "Training - epoch: 6, final step mse: 0.0029, total mse: 0.0029\n",
      "Time - 22.747468948364258\n",
      "Testing - epoch: 6,  final step mse: 0.2254, total mse: 0.0029\n",
      "Training - epoch: 7, final step mse: 0.0028, total mse: 0.0028\n",
      "Time - 22.750892400741577\n",
      "Testing - epoch: 7,  final step mse: 0.2255, total mse: 0.0028\n",
      "Training - epoch: 8, final step mse: 0.0026, total mse: 0.0026\n",
      "Time - 22.730112314224243\n",
      "Testing - epoch: 8,  final step mse: 0.2258, total mse: 0.0027\n",
      "Training - epoch: 9, final step mse: 0.0025, total mse: 0.0025\n",
      "Time - 22.731621265411377\n",
      "Testing - epoch: 9,  final step mse: 0.2255, total mse: 0.0026\n",
      "Training - epoch: 10, final step mse: 0.0024, total mse: 0.0024\n",
      "Time - 22.81556272506714\n",
      "Testing - epoch: 10,  final step mse: 0.2261, total mse: 0.0025\n",
      "Training - epoch: 11, final step mse: 0.0023, total mse: 0.0023\n",
      "Time - 22.774198532104492\n",
      "Testing - epoch: 11,  final step mse: 0.2258, total mse: 0.0024\n",
      "Training - epoch: 12, final step mse: 0.0022, total mse: 0.0022\n",
      "Time - 22.76768159866333\n",
      "Testing - epoch: 12,  final step mse: 0.2258, total mse: 0.0023\n",
      "Training - epoch: 13, final step mse: 0.0021, total mse: 0.0021\n",
      "Time - 22.750062704086304\n",
      "Testing - epoch: 13,  final step mse: 0.2262, total mse: 0.0023\n",
      "Training - epoch: 14, final step mse: 0.0021, total mse: 0.0021\n",
      "Time - 22.72073531150818\n",
      "Testing - epoch: 14,  final step mse: 0.2266, total mse: 0.0022\n",
      "Training - epoch: 15, final step mse: 0.0020, total mse: 0.0020\n",
      "Time - 22.767969608306885\n",
      "Testing - epoch: 15,  final step mse: 0.2264, total mse: 0.0022\n",
      "Training - epoch: 16, final step mse: 0.0020, total mse: 0.0020\n",
      "Time - 22.77094864845276\n",
      "Testing - epoch: 16,  final step mse: 0.2264, total mse: 0.0022\n",
      "Training - epoch: 17, final step mse: 0.0019, total mse: 0.0019\n",
      "Time - 22.735729932785034\n",
      "Testing - epoch: 17,  final step mse: 0.2271, total mse: 0.0021\n",
      "Training - epoch: 18, final step mse: 0.0019, total mse: 0.0019\n",
      "Time - 22.767120838165283\n",
      "Testing - epoch: 18,  final step mse: 0.2265, total mse: 0.0021\n",
      "Training - epoch: 19, final step mse: 0.0019, total mse: 0.0019\n",
      "Time - 22.787415981292725\n",
      "Testing - epoch: 19,  final step mse: 0.2266, total mse: 0.0021\n",
      "Training - epoch: 20, final step mse: 0.0018, total mse: 0.0018\n",
      "Time - 22.754135370254517\n",
      "Testing - epoch: 20,  final step mse: 0.2268, total mse: 0.0020\n",
      "Training - epoch: 21, final step mse: 0.0018, total mse: 0.0018\n",
      "Time - 22.70596718788147\n",
      "Testing - epoch: 21,  final step mse: 0.2261, total mse: 0.0020\n",
      "Training - epoch: 22, final step mse: 0.0018, total mse: 0.0018\n",
      "Time - 22.805418252944946\n",
      "Testing - epoch: 22,  final step mse: 0.2266, total mse: 0.0020\n",
      "Training - epoch: 23, final step mse: 0.0018, total mse: 0.0018\n",
      "Time - 22.79681944847107\n",
      "Testing - epoch: 23,  final step mse: 0.2265, total mse: 0.0020\n",
      "Training - epoch: 24, final step mse: 0.0017, total mse: 0.0017\n",
      "Time - 22.810889720916748\n",
      "Testing - epoch: 24,  final step mse: 0.2266, total mse: 0.0019\n",
      "Training - epoch: 25, final step mse: 0.0017, total mse: 0.0017\n",
      "Time - 22.718130111694336\n",
      "Testing - epoch: 25,  final step mse: 0.2269, total mse: 0.0019\n",
      "Training - epoch: 26, final step mse: 0.0017, total mse: 0.0017\n",
      "Time - 22.76168203353882\n",
      "Testing - epoch: 26,  final step mse: 0.2264, total mse: 0.0019\n",
      "Training - epoch: 27, final step mse: 0.0017, total mse: 0.0017\n",
      "Time - 22.736910581588745\n",
      "Testing - epoch: 27,  final step mse: 0.2268, total mse: 0.0019\n",
      "Training - epoch: 28, final step mse: 0.0017, total mse: 0.0017\n",
      "Time - 22.844870805740356\n",
      "Testing - epoch: 28,  final step mse: 0.2261, total mse: 0.0019\n",
      "Training - epoch: 29, final step mse: 0.0016, total mse: 0.0016\n",
      "Time - 22.805057764053345\n",
      "Testing - epoch: 29,  final step mse: 0.2272, total mse: 0.0019\n",
      "Training - epoch: 30, final step mse: 0.0016, total mse: 0.0016\n",
      "Time - 22.738136291503906\n",
      "Testing - epoch: 30,  final step mse: 0.2260, total mse: 0.0018\n",
      "Training - epoch: 31, final step mse: 0.0016, total mse: 0.0016\n",
      "Time - 22.76422429084778\n",
      "Testing - epoch: 31,  final step mse: 0.2259, total mse: 0.0018\n",
      "Training - epoch: 32, final step mse: 0.0016, total mse: 0.0016\n",
      "Time - 22.799828052520752\n",
      "Testing - epoch: 32,  final step mse: 0.2275, total mse: 0.0018\n",
      "Training - epoch: 33, final step mse: 0.0016, total mse: 0.0016\n",
      "Time - 22.73421287536621\n",
      "Testing - epoch: 33,  final step mse: 0.2271, total mse: 0.0018\n",
      "Training - epoch: 34, final step mse: 0.0016, total mse: 0.0016\n",
      "Time - 22.79025912284851\n",
      "Testing - epoch: 34,  final step mse: 0.2264, total mse: 0.0018\n",
      "Training - epoch: 35, final step mse: 0.0016, total mse: 0.0016\n",
      "Time - 22.822110414505005\n",
      "Testing - epoch: 35,  final step mse: 0.2272, total mse: 0.0018\n",
      "Training - epoch: 36, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.757447481155396\n",
      "Testing - epoch: 36,  final step mse: 0.2265, total mse: 0.0018\n",
      "Training - epoch: 37, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.787468194961548\n",
      "Testing - epoch: 37,  final step mse: 0.2271, total mse: 0.0017\n",
      "Training - epoch: 38, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.715128898620605\n",
      "Testing - epoch: 38,  final step mse: 0.2265, total mse: 0.0017\n",
      "Training - epoch: 39, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.738364219665527\n",
      "Testing - epoch: 39,  final step mse: 0.2266, total mse: 0.0018\n",
      "Training - epoch: 40, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.748753309249878\n",
      "Testing - epoch: 40,  final step mse: 0.2261, total mse: 0.0017\n",
      "Training - epoch: 41, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.810593128204346\n",
      "Testing - epoch: 41,  final step mse: 0.2269, total mse: 0.0017\n",
      "Training - epoch: 42, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.807981967926025\n",
      "Testing - epoch: 42,  final step mse: 0.2269, total mse: 0.0017\n",
      "Training - epoch: 43, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.739955186843872\n",
      "Testing - epoch: 43,  final step mse: 0.2267, total mse: 0.0017\n",
      "Training - epoch: 44, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.81417751312256\n",
      "Testing - epoch: 44,  final step mse: 0.2269, total mse: 0.0017\n",
      "Training - epoch: 45, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.73703646659851\n",
      "Testing - epoch: 45,  final step mse: 0.2274, total mse: 0.0017\n",
      "Training - epoch: 46, final step mse: 0.0015, total mse: 0.0015\n",
      "Time - 22.70997428894043\n",
      "Testing - epoch: 46,  final step mse: 0.2266, total mse: 0.0017\n",
      "Training - epoch: 47, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.782758474349976\n",
      "Testing - epoch: 47,  final step mse: 0.2269, total mse: 0.0017\n",
      "Training - epoch: 48, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.707218885421753\n",
      "Testing - epoch: 48,  final step mse: 0.2262, total mse: 0.0017\n",
      "Training - epoch: 49, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.728806257247925\n",
      "Testing - epoch: 49,  final step mse: 0.2267, total mse: 0.0017\n",
      "Training - epoch: 50, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.770148754119873\n",
      "Testing - epoch: 50,  final step mse: 0.2270, total mse: 0.0017\n",
      "Training - epoch: 51, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.738112688064575\n",
      "Testing - epoch: 51,  final step mse: 0.2268, total mse: 0.0017\n",
      "Training - epoch: 52, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.79405665397644\n",
      "Testing - epoch: 52,  final step mse: 0.2267, total mse: 0.0017\n",
      "Training - epoch: 53, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.76974868774414\n",
      "Testing - epoch: 53,  final step mse: 0.2270, total mse: 0.0017\n",
      "Training - epoch: 54, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.775768518447876\n",
      "Testing - epoch: 54,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 55, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.747061014175415\n",
      "Testing - epoch: 55,  final step mse: 0.2266, total mse: 0.0016\n",
      "Training - epoch: 56, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.84088373184204\n",
      "Testing - epoch: 56,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 57, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.7549946308136\n",
      "Testing - epoch: 57,  final step mse: 0.2271, total mse: 0.0016\n",
      "Training - epoch: 58, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.73617458343506\n",
      "Testing - epoch: 58,  final step mse: 0.2264, total mse: 0.0016\n",
      "Training - epoch: 59, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.761765003204346\n",
      "Testing - epoch: 59,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 60, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.758493185043335\n",
      "Testing - epoch: 60,  final step mse: 0.2271, total mse: 0.0016\n",
      "Training - epoch: 61, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.773531675338745\n",
      "Testing - epoch: 61,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 62, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.77719759941101\n",
      "Testing - epoch: 62,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 63, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.803571462631226\n",
      "Testing - epoch: 63,  final step mse: 0.2268, total mse: 0.0016\n",
      "Training - epoch: 64, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.755735635757446\n",
      "Testing - epoch: 64,  final step mse: 0.2268, total mse: 0.0016\n",
      "Training - epoch: 65, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.838343143463135\n",
      "Testing - epoch: 65,  final step mse: 0.2277, total mse: 0.0016\n",
      "Training - epoch: 66, final step mse: 0.0014, total mse: 0.0014\n",
      "Time - 22.698520183563232\n",
      "Testing - epoch: 66,  final step mse: 0.2270, total mse: 0.0016\n",
      "Training - epoch: 67, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.811249494552612\n",
      "Testing - epoch: 67,  final step mse: 0.2271, total mse: 0.0016\n",
      "Training - epoch: 68, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.793161153793335\n",
      "Testing - epoch: 68,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 69, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.81117820739746\n",
      "Testing - epoch: 69,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 70, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.816856384277344\n",
      "Testing - epoch: 70,  final step mse: 0.2267, total mse: 0.0016\n",
      "Training - epoch: 71, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.804068565368652\n",
      "Testing - epoch: 71,  final step mse: 0.2268, total mse: 0.0016\n",
      "Training - epoch: 72, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.78898525238037\n",
      "Testing - epoch: 72,  final step mse: 0.2271, total mse: 0.0016\n",
      "Training - epoch: 73, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.783493041992188\n",
      "Testing - epoch: 73,  final step mse: 0.2266, total mse: 0.0016\n",
      "Training - epoch: 74, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.806370973587036\n",
      "Testing - epoch: 74,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 75, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.74486231803894\n",
      "Testing - epoch: 75,  final step mse: 0.2271, total mse: 0.0016\n",
      "Training - epoch: 76, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.815200090408325\n",
      "Testing - epoch: 76,  final step mse: 0.2272, total mse: 0.0016\n",
      "Training - epoch: 77, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.75869846343994\n",
      "Testing - epoch: 77,  final step mse: 0.2268, total mse: 0.0016\n",
      "Training - epoch: 78, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.747798442840576\n",
      "Testing - epoch: 78,  final step mse: 0.2274, total mse: 0.0016\n",
      "Training - epoch: 79, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.74527645111084\n",
      "Testing - epoch: 79,  final step mse: 0.2274, total mse: 0.0016\n",
      "Training - epoch: 80, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.82193422317505\n",
      "Testing - epoch: 80,  final step mse: 0.2267, total mse: 0.0016\n",
      "Training - epoch: 81, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.823418617248535\n",
      "Testing - epoch: 81,  final step mse: 0.2269, total mse: 0.0016\n",
      "Training - epoch: 82, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.760281085968018\n",
      "Testing - epoch: 82,  final step mse: 0.2268, total mse: 0.0016\n",
      "Training - epoch: 83, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.802974939346313\n",
      "Testing - epoch: 83,  final step mse: 0.2270, total mse: 0.0016\n",
      "Training - epoch: 84, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.84404969215393\n",
      "Testing - epoch: 84,  final step mse: 0.2274, total mse: 0.0016\n",
      "Training - epoch: 85, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.754862308502197\n",
      "Testing - epoch: 85,  final step mse: 0.2268, total mse: 0.0016\n",
      "Training - epoch: 86, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.77789616584778\n",
      "Testing - epoch: 86,  final step mse: 0.2272, total mse: 0.0016\n",
      "Training - epoch: 87, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.796891450881958\n",
      "Testing - epoch: 87,  final step mse: 0.2272, total mse: 0.0016\n",
      "Training - epoch: 88, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.767996072769165\n",
      "Testing - epoch: 88,  final step mse: 0.2271, total mse: 0.0016\n",
      "Training - epoch: 89, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.730661392211914\n",
      "Testing - epoch: 89,  final step mse: 0.2266, total mse: 0.0016\n",
      "Training - epoch: 90, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.77900195121765\n",
      "Testing - epoch: 90,  final step mse: 0.2268, total mse: 0.0016\n",
      "Training - epoch: 91, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.875527143478394\n",
      "Testing - epoch: 91,  final step mse: 0.2275, total mse: 0.0016\n",
      "Training - epoch: 92, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.82975721359253\n",
      "Testing - epoch: 92,  final step mse: 0.2276, total mse: 0.0016\n",
      "Training - epoch: 93, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.791962146759033\n",
      "Testing - epoch: 93,  final step mse: 0.2265, total mse: 0.0015\n",
      "Training - epoch: 94, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.825201511383057\n",
      "Testing - epoch: 94,  final step mse: 0.2267, total mse: 0.0016\n",
      "Training - epoch: 95, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.871917724609375\n",
      "Testing - epoch: 95,  final step mse: 0.2267, total mse: 0.0015\n",
      "Training - epoch: 96, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.793155670166016\n",
      "Testing - epoch: 96,  final step mse: 0.2263, total mse: 0.0015\n",
      "Training - epoch: 97, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.803102016448975\n",
      "Testing - epoch: 97,  final step mse: 0.2277, total mse: 0.0016\n",
      "Training - epoch: 98, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.765827417373657\n",
      "Testing - epoch: 98,  final step mse: 0.2272, total mse: 0.0015\n",
      "Training - epoch: 99, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.80172562599182\n",
      "Testing - epoch: 99,  final step mse: 0.2274, total mse: 0.0015\n",
      "Training - epoch: 100, final step mse: 0.0013, total mse: 0.0013\n",
      "Time - 22.805500745773315\n",
      "Testing - epoch: 100,  final step mse: 0.2273, total mse: 0.0015\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    start = Time.time()\n",
    "    state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "    end = Time.time()\n",
    "    print(f'Time - {end - start}')\n",
    "    eval_model(state.params, test_ds)\n",
    "    dict_output = serialization.to_state_dict(state.params)\n",
    "    np.save(f\"checkpoint/CNN{n_look_ahead}_state_dict_epoch_{epoch}.npy\", dict_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6654b1-23c7-41d2-bf2c-f6c16e54dfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
